<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Project Overview</title>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
</head>
<body>
  <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    <div class="container-fluid">
      <a class="navbar-brand" href="/">Sentiment Analyzer</a>
      <div class="navbar-nav">
        <a class="nav-link" href="/">Home</a>
        <a class="nav-link" href="/compare">Compare Models</a>
        <a class="nav-link active" href="/links">Project Page</a>
      </div>
    </div>
  </nav>

  <div class="container mt-5">
    <div class="card shadow p-4">
      <h2 class="text-center mb-4">Sentiment Analysis Project Overview</h2>
      
      <p>This project analyzes the sentiment of textual data using multiple machine learning models. The main goal is to classify input text as having <strong>positive</strong> or <strong>negative</strong> sentiment using a trained classifier.</p>

      <h4>üìò Theory</h4>
      <p><strong>How It Works:</strong> Sentiment Analysis is a branch of Natural Language Processing (NLP) that deals with identifying and categorizing emotions in text. The process begins with preprocessing‚Äîremoving stopwords, punctuation, and irrelevant characters‚Äîfollowed by tokenization and feature extraction using techniques like TF-IDF (Term Frequency-Inverse Document Frequency). These extracted features are used to train supervised learning models that can identify sentiment patterns from the data.</p>

      <p>In our project, we use TF-IDF to transform tweets into numerical vectors and apply multiple classifiers to predict whether a selected portion of a tweet conveys a positive or negative sentiment. The project demonstrates the complete pipeline from raw data to prediction and visualization.</p>

      <p><strong>Applications:</strong> Sentiment analysis is used extensively across industries. Social media platforms monitor public opinion trends, businesses analyze customer feedback, and financial firms assess market sentiment. It is also employed in areas like brand monitoring and mental health analysis. Our project emulates these real-world applications using a diverse tweet-based dataset enhanced with demographic and geographic metadata.</p>

      <p><strong>Project Functionality:</strong> The web application lets users input text and view sentiment predictions from four different machine learning models. It also visualizes model accuracies and links to a Jupyter notebook that contains detailed theoretical background, code, and evaluation. The backend is developed using Flask, and the frontend uses Bootstrap for a responsive and intuitive UI.</p>

      <h4>üîç Model Implementation & Insights</h4>

<p>In our project, each machine learning model was implemented using <strong>scikit-learn</strong> or <strong>XGBoost</strong>, with TF-IDF-transformed features serving as input. Here's how each model was integrated into the pipeline and how it performed based on our dataset:</p>

<h5>1. üìà Logistic Regression</h5>
<p>Logistic Regression is a linear model that works well for binary classification tasks. We trained this model using the sentiment-labeled tweets, where the TF-IDF vectors represented the features and the sentiment labels served as the target. This model achieved the <strong>highest accuracy of 83.5%</strong>, making it the most reliable among the models used. Its simplicity, interpretability, and generalization ability made it ideal for this task.</p>

<p><strong>Why it worked:</strong> Sentiment classification often follows linear boundaries in the TF-IDF feature space, and Logistic Regression is well-suited for this. It handled class imbalance and high-dimensional sparse features effectively.</p>

<h5>2. üìâ Linear Regression</h5>
<p>Linear Regression is typically used for predicting continuous values, but we experimented with it to observe how a regression model would behave when forced to make a classification decision (by applying a threshold). However, this model yielded a very poor accuracy of <strong>19.65%</strong>. It was clearly unsuitable for a classification task due to its inability to distinguish class boundaries effectively.</p>

<p><strong>Why it failed:</strong> Since Linear Regression minimizes squared error and is not optimized for classification, it produced continuous values that didn‚Äôt map well to categorical sentiment labels. This was a benchmark case to demonstrate model selection importance.</p>

<h5>3. üßÆ Multinomial Naive Bayes</h5>
<p>Multinomial Naive Bayes is a probabilistic model that assumes feature independence and works well with word frequency data. It achieved an accuracy of <strong>78.4%</strong>. Although slightly less accurate than Logistic Regression, it still performed quite well given its simplicity and speed.</p>

<p><strong>Why it worked:</strong> The TF-IDF features, although not pure counts, worked decently with the Naive Bayes assumption. Its performance is strong in text classification tasks due to the model‚Äôs strength in handling high-dimensional and sparse data.</p>

<h5>4. üöÄ XGBoost Classifier</h5>
<p>XGBoost is a powerful ensemble method based on gradient boosting. It learns complex non-linear patterns and handles overfitting using regularization. It achieved an accuracy of <strong>80.69%</strong>, making it the second-best performing model in our project.</p>

<p><strong>Why it worked:</strong> XGBoost excels at handling tabular data with mixed types and can capture intricate relationships in data. With proper parameter tuning and early stopping, it provided high predictive power without significant overfitting.</p>

<h5>5. üß† Support Vector Machine (SVM)</h5>
<p>SVM is a powerful classification algorithm that works by finding the optimal hyperplane that separates classes in high-dimensional space. We used an SVM with a linear kernel for our dataset. It achieved the <strong>highest accuracy of 84.05%</strong>, making it the top performer in our sentiment classification task.</p>

<p><strong>Why it worked:</strong> SVM is particularly effective for high-dimensional text data. Its ability to handle sparse TF-IDF vectors and find a decision boundary with maximum margin helped improve classification robustness.</p>

<h5>6. üå≤ Random Forest Classifier</h5>
<p>Random Forest is an ensemble of decision trees that improves accuracy by reducing overfitting. It achieved an accuracy of <strong>80.32%</strong>, placing it in the upper tier of our tested models.</p>

<p><strong>Why it worked:</strong> By averaging predictions across multiple trees trained on different data subsets and features, Random Forest reduced variance and maintained strong performance across a diverse dataset.</p>

<h5>7. üß≠ K-Nearest Neighbors (KNN)</h5>
<p>KNN is a simple instance-based learning algorithm that classifies a sample based on the majority sentiment of its nearest neighbors. While straightforward, it achieved an accuracy of <strong>69.41%</strong>, making it the least accurate of the viable models used (excluding Linear Regression).</p>

<p><strong>Why it worked (to an extent):</strong> KNN works better with fewer, more distinguishable features. The high-dimensional nature of TF-IDF vectors made distance calculations less effective, but it still offered a reasonable baseline performance.</p>

<h5>‚úÖ Summary of Usefulness:</h5>
<ul>
  <li><strong>‚úî Logistic Regression:</strong> Best performing and most balanced ‚Äî used as a benchmark.</li>
  <li><strong>‚úñ Linear Regression:</strong> Poor choice for classification ‚Äî included for contrast and academic interest.</li>
  <li><strong>‚úî Naive Bayes:</strong> Simple, fast, and effective ‚Äî good for baseline or quick inference.</li>
  <li><strong>‚úî XGBoost:</strong> Powerful and accurate ‚Äî slightly more complex but robust.</li>
  <li><strong>‚úî SVM:</strong> Most accurate and robust ‚Äî excellent choice for high-dimensional text data.</li>
  <li><strong>‚úî Random Forest:</strong> Strong performance ‚Äî good balance between complexity and generalization.</li>
  <li><strong>‚ûñ KNN:</strong> Decent accuracy but less suitable for high-dimensional data ‚Äî useful for academic purposes or small-scale tasks.</li>
</ul>

<p>Each model was saved using <code>joblib</code> and loaded in <code>app.py</code> to make real-time predictions through the web interface. Users can view model-wise predictions and compare their results via the 'Compare Models' tab.</p>


      <h4>üìä Model Accuracy</h4>
      <ul>
        <li><strong>Logistic Regression:</strong> 83.5%</li>
        <li><strong>Linear Regression:</strong> 19.65%</li>
        <li><strong>Naive Bayes:</strong> 78.4%</li>
        <li><strong>XGBoost:</strong> 80.69%</li>
        <li><strong>SVM:</strong> 84.05%</li>
        <li><strong>Random Forest:</strong> 80.32%</li>
        <li><strong>KNN:</strong> 69.41%</li>
      </ul>

      <h4>üìÅ Dataset Details</h4>
      <p><strong>Source:</strong> <a href="https://www.kaggle.com/datasets/abhi8923shriv/sentiment-analysis-dataset" target="_blank">Kaggle - Sentiment Analysis Dataset</a></p>

      <p><strong>Description:</strong> The dataset consists of tweets annotated with sentiment labels and supplemented with demographic and geographic information. Each entry includes:</p>
      <ul>
        <li><strong>textID:</strong> Unique identifier for each tweet</li>
        <li><strong>Text:</strong> Full tweet content</li>
        <li><strong>selected_Text:</strong> Highlighted portion of the tweet expressing sentiment</li>
        <li><strong>sentiment:</strong> Sentiment label (neutral, positive, negative)</li>
        <li><strong>Time of Tweet:</strong> Categorized as morning, noon, or evening/night</li>
        <li><strong>Age of User:</strong> 0-20, 21-30, or others</li>
        <li><strong>Country:</strong> User's country of origin</li>
        <li><strong>Population:</strong> Population of the user's country</li>
        <li><strong>Land Area:</strong> Land area of the country</li>
        <li><strong>Density:</strong> Population density</li>
      </ul>

      <p><strong>Training Columns:</strong> Polarity, Tweet ID, Date, Query, User, Text</p>
      <p><strong>Dataset Size:</strong> 838,857 training tweets and 209,715 test tweets</p>

      <h4>üìÇ Project Structure</h4>
      <pre>
Sentiment_Analysis_PRML_Project/
‚îÇ
‚îú‚îÄ‚îÄ app.py                      # Main Flask application
‚îú‚îÄ‚îÄ model/                      # Folder containing saved models and vectorizer
‚îÇ   ‚îú‚îÄ‚îÄ log_reg.pkl
‚îÇ   ‚îú‚îÄ‚îÄ lin_reg.pkl
‚îÇ   ‚îú‚îÄ‚îÄ nb_model.pkl
‚îÇ   ‚îú‚îÄ‚îÄ xgb_model.pkl
‚îÇ   ‚îî‚îÄ‚îÄ vectorizer.pkl
‚îÇ
‚îú‚îÄ‚îÄ templates/                  # HTML templates
‚îÇ   ‚îú‚îÄ‚îÄ index.html
‚îÇ   ‚îú‚îÄ‚îÄ result.html
‚îÇ   ‚îú‚îÄ‚îÄ compare.html
‚îÇ   ‚îî‚îÄ‚îÄ links.html
‚îÇ
‚îú‚îÄ‚îÄ static/                     # CSS, images, if any
‚îú‚îÄ‚îÄ Copy_of_Project.ipynb       # Notebook with full code and EDA
‚îî‚îÄ‚îÄ README.md                   # Project summary
      </pre>

      <h4>üìö Materials and Libraries Used</h4>
      <ul>
        <li>TF-IDF Vectorizer</li>
        <li>scikit-learn</li>
        <li>XGBoost</li>
        <li>Flask</li>
        <li>Jupyter Notebook</li>
      </ul>

      <h4>üîó Useful Links</h4>
      <ul>
        <li><a href="https://github.com/ParthKhiriya/Sentiment_Analysis_PRML_Project" target="_blank">GitHub Repository</a></li>
        <li><a href="https://www.kaggle.com/datasets/abhi8923shriv/sentiment-analysis-dataset" target="_blank">Kaggle Dataset</a></li>
        <li><a href="https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction" target="_blank">TF-IDF Vectorizer (scikit-learn)</a></li>
        <li><a href="https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression" target="_blank">Logistic Regression - scikit-learn Docs</a></li>
        <li><a href="https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares" target="_blank">Linear Regression - scikit-learn Docs</a></li>
        <li><a href="https://scikit-learn.org/stable/modules/naive_bayes.html" target="_blank">Naive Bayes - scikit-learn Docs</a></li>
        <li><a href="https://xgboost.readthedocs.io/en/stable/" target="_blank">XGBoost Documentation</a></li>
        <li><a href="https://flask.palletsprojects.com/en/2.3.x/" target="_blank">Flask Documentation</a></li>
        <li><a href="https://realpython.com/nltk-nlp-python/" target="_blank">NLP with Python (Real Python Guide)</a></li>
        <li><a href="https://www.analyticsvidhya.com/blog/2021/06/sentiment-analysis-using-python/" target="_blank">Sentiment Analysis Tutorial (Analytics Vidhya)</a></li>
      </ul>

    </div>
  </div>
</body>
</html>
