# -*- coding: utf-8 -*-
"""New_PRML_Sentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13tkU0oFFodttyJsTV4FME5DWDdR7EKnR
"""

import pandas as pd
import numpy as np
import re
import string
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib import style
style.use('ggplot')
from textblob import TextBlob
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
from wordcloud import WordCloud
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay

# Installing the kaggle library
! pip install kaggle

# configuring the path of kaggle.json file
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Fetching the API to download the dataset
! kaggle datasets download -d abhi8923shriv/sentiment-analysis-dataset

from zipfile import ZipFile
dataset = '/content/sentiment-analysis-dataset.zip'

with ZipFile(dataset,'r') as zip:
  zip.extractall()
  print('The dataset is extracted')

# Loading the main dataset and also the train and test set
df_train = pd.read_csv('train.csv', encoding='latin-1', header= None)
df_train = df_train.drop(labels=0)
df_train.columns = ["textID","text","selected_text","sentiment","Time of Tweet","Age of User","Country","Population -2020","Land Area (Km²)","Density (P/Km²)"]

# This gives the information about our data,  like what are the datatypes of content present in all the columns and also the null count
df_train = df_train.drop(["textID","selected_text","Time of Tweet","Age of User","Country","Population -2020","Land Area (Km²)","Density (P/Km²)"], axis=1)
df_train = df_train.dropna()
print(df_train)

# Loading the main dataset and also the train and test set
df_test = pd.read_csv('test.csv', encoding='latin-1', header= None)
df_test = df_test.drop(labels=0)
df_test.columns = ["textID","text","sentiment","Time of Tweet","Age of User","Country","Population -2020","Land Area (Km²)","Density (P/Km²)"]

# This gives the information about our data,  like what are the datatypes of content present in all the columns and also the null count
df_test = df_test.drop(["textID","Time of Tweet","Age of User","Country","Population -2020","Land Area (Km²)","Density (P/Km²)"], axis=1)
df_test = df_test.dropna()
print(df_test)

df = pd.concat([df_train, df_test], ignore_index=True)

# Shuffle the dataset (good practice before splitting again)
df = df.sample(frac=1, random_state=42).reset_index(drop=True)

# Showing basic info
print(df.shape)
print(df.head())

sentiment_ordering = ['negative', 'neutral', 'positive']

df["sentiment"] = df["sentiment"].apply(lambda x: sentiment_ordering.index(x))
print(df)

df['sentiment'].value_counts()

# Since value_counts() is showing two different types of 0s so we have to check what is the problem
print(df['sentiment'].unique())
print(df['sentiment'].value_counts())

# This function performs basic tasks for cleaning the data such as lowercasing, URL handling etc.
def clean_text(text):

    text = text.lower()  # Lowercasing
    text = re.sub(r'http\S+|www.\S+', '', text)  # Remove URLs
    text = re.sub(r'@\w+|#\w+', '', text)  # Remove mentions and hashtags
    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation
    text = re.sub(r'\d+', '', text)  # Remove numbers
    return text.strip()

# Save the cleaned text in our dataframe
df['cleaned_text'] = df['text'].apply(clean_text)

# Tokenisation - splits the dataset into an array of words(nltk generally performs word tokenisation i.e. seperating words) for better handling of punctuations
df['tokens'] = df['cleaned_text'].apply(lambda x: x.split())

# Removing all the stopwords because it will not impact the sentiment of the tweet
stop_words = set(stopwords.words('english'))
df['tokens'] = df['tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])

# Then stemming(converting a word into its root word e.g. acting, actor to act)
stemmer = PorterStemmer()
df['stemmed_text'] = df['tokens'].apply(lambda tokens: ' '.join([stemmer.stem(word) for word in tokens]))

print(df)

from sklearn.model_selection import train_test_split

X = df['stemmed_text']
y = df['sentiment']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify= y, random_state=42)

print(X.shape, X_train.shape, X_test.shape)

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
X_train = vectorizer.fit_transform(X_train)
X_test = vectorizer.transform(X_test)

print(f"Training Data:\n {X_train}")
print(f"Testing Data:\n {X_test}")

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train, y_train)

y_pred_train_logistic = log_reg.predict(X_train)
accuracy_train_logistic = accuracy_score(y_train, y_pred_train_logistic)
print(f"Training accuracy : {accuracy_train_logistic}")

y_pred_test_logistic = log_reg.predict(X_test)
accuracy_test_logistic = accuracy_score(y_test, y_pred_test_logistic)
print(f"Testing accuracy : {accuracy_test_logistic}")

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)

y_pred_train_linear = lin_reg.predict(X_train)
r2_train_linear = accuracy_score(y_train, y_pred_train_linear)
print(f"Training accuracy : {r2_train_linear}")

y_pred_test_linear = lin_reg.predict(X_test)
r2_test_linear = accuracy_score(y_test, y_pred_test_linear)
print(f"Testing accuracy : {r2_test_linear}")

from sklearn.svm import SVC

svm_model = SVC(kernel='linear')
svm_model.fit(X_train, y_train)

y_pred_svm = svm_model.predict(X_test)
print("SVM Accuracy:", accuracy_score(y_test, y_pred_svm))

from sklearn.naive_bayes import MultinomialNB

nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)

y_pred_nb = nb_model.predict(X_test)
print("Naïve Bayes Accuracy:", accuracy_score(y_test, y_pred_nb))

from xgboost import XGBClassifier

xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb_model.fit(X_train, y_train)

y_pred_xgb = xgb_model.predict(X_test)
print("XGBoost Accuracy:", accuracy_score(y_test, y_pred_xgb))

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report

# Initialize the Decision Tree Classifier
dt_clf = DecisionTreeClassifier(random_state=42)

# Train the model
dt_clf.fit(X_train, y_train)

# Predictions
y_pred_train_dt = dt_clf.predict(X_train)
y_pred_test_dt = dt_clf.predict(X_test)

# Evaluate the model
accuracy_train_dt = accuracy_score(y_train, y_pred_train_dt)
accuracy_test_dt = accuracy_score(y_test, y_pred_test_dt)

print(f"Decision Tree Training Accuracy: {accuracy_train_dt}")
print(f"Decision Tree Testing Accuracy: {accuracy_test_dt}")

# Classification report
print("Classification Report for Decision Tree on Test Set:")
print(classification_report(y_test, y_pred_test_dt))

from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

y_pred_rf = rf_model.predict(X_test)
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))

import joblib

# Save existing models
joblib.dump(log_reg, 'log_reg.pkl')
joblib.dump(lin_reg, 'lin_reg.pkl')
joblib.dump(nb_model, 'nb_model.pkl')
joblib.dump(xgb_model, 'xgb_model.pkl')
joblib.dump(rf_model, 'rf_model.pkl')
joblib.dump(svm_model, 'svm_model.pkl')
joblib.dump(dt_clf, 'dt_model.pkl')



# Save the shared vectorizer
joblib.dump(vectorizer, 'vectorizer.pkl')

from google.colab import files

# Existing models
files.download('log_reg.pkl')
files.download('lin_reg.pkl')
files.download('nb_model.pkl')
files.download('xgb_model.pkl')
files.download('rf_model.pkl')
files.download('svm_model.pkl')
files.download('dt_model.pkl')

# Shared vectorizer
files.download('vectorizer.pkl')

from sklearn.metrics import accuracy_score, r2_score

# Existing models
y_pred_log = log_reg.predict(X_test)
acc_log = accuracy_score(y_test, y_pred_log)

y_pred_lin = lin_reg.predict(X_test)
acc_lin = r2_score(y_test, y_pred_lin)  # Regression model

y_pred_nb = nb_model.predict(X_test)
acc_nb = accuracy_score(y_test, y_pred_nb)

y_pred_xgb = xgb_model.predict(X_test)
acc_xgb = accuracy_score(y_test, y_pred_xgb)

y_pred_rf = rf_model.predict(X_test)
acc_rf = accuracy_score(y_test, y_pred_rf)

y_pred_svm = svm_model.predict(X_test)
acc_svm = accuracy_score(y_test, y_pred_svm)

y_pred_dt = dt_clf.predict(X_test)
acc_dt = accuracy_score(y_test, y_pred_dt)

accuracies = {
    'Logistic Regression': acc_log,
    'Linear Regression (R²)': acc_lin,
    'Naive Bayes': acc_nb,
    'XGBoost': acc_xgb,
    'Random Forest': acc_rf,
    'SVM': acc_svm,
    'Decision Tree': acc_dt
}

# Save as pkl
joblib.dump(accuracies, 'accuracies.pkl')


# For downloading in Jupyter Notebook
from google.colab import files
files.download('accuracies.pkl')
